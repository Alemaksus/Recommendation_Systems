{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Для работы с матрицами\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Матричная факторизация\n",
    "from implicit import als\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Модель второго уровня\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(1, os.getcwd() + '/src/')\n",
    "\n",
    "from metrics import precision_at_k, recall_at_k\n",
    "from utils import prefilter_items\n",
    "from recommenders import MainRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('retail_train.csv')\n",
    "item_features = pd.read_csv('product.csv')\n",
    "user_features = pd.read_csv('hh_demographic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several important functions:\n",
    "\n",
    "def print_stats_data(df_data, name_df):\n",
    "    print(name_df)\n",
    "    print(f\"Shape: {df_data.shape} Users: {df_data[USER_COL].nunique()} Items: {df_data[ITEM_COL].nunique()}\")\n",
    "    \n",
    "def make_recommendations(df_result, recommend_model, N_PREDICT=500, USER_COL='user_id'):\n",
    "    return df_result[USER_COL].apply(lambda x: recommend_model(x, N=N_PREDICT))\n",
    "\n",
    "def calc_recall(df_data, top_k, ACTUAL_COL='actual'):\n",
    "    for col_name in df_data.columns[2:]:\n",
    "        yield col_name, df_data.apply(lambda row: recall_at_k(row[col_name], row[ACTUAL_COL], k=top_k), axis=1).mean()\n",
    "        \n",
    "def calc_precision(df_data, top_k, ACTUAL_COL='actual'):\n",
    "    for col_name in df_data.columns[2:]:\n",
    "        yield col_name, df_data.apply(lambda row: precision_at_k(row[col_name], row[ACTUAL_COL], k=top_k), axis=1).mean()\n",
    "        \n",
    "def rerank(user_id, df, USER_COL='user_id', proba_col_name='proba_item_purchase', N=5):\n",
    "    return df[df[USER_COL]==user_id].sort_values(proba_col_name, ascending=False).head(N).item_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data preparation\n",
    "\n",
    "ITEM_COL = 'item_id'\n",
    "USER_COL = 'user_id'\n",
    "\n",
    "# column processing\n",
    "item_features.columns = [col.lower() for col in item_features.columns]\n",
    "user_features.columns = [col.lower() for col in user_features.columns]\n",
    "\n",
    "item_features.rename(columns={'product_id': ITEM_COL}, inplace=True)\n",
    "user_features.rename(columns={'household_key': USER_COL }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_MATCHER_WEEKS = 5\n",
    "VAL_RANKER_WEEKS = 3\n",
    "\n",
    "# берем данные для тренировки matching модели\n",
    "data_train_matcher = data[data['week_no'] < data['week_no'].max() - (VAL_MATCHER_WEEKS + VAL_RANKER_WEEKS)]\n",
    "\n",
    "# берем данные для валидации matching модели\n",
    "data_val_matcher = data[(data['week_no'] >= data['week_no'].max() - (VAL_MATCHER_WEEKS + VAL_RANKER_WEEKS)) &\n",
    "                      (data['week_no'] < data['week_no'].max() - (VAL_RANKER_WEEKS))]\n",
    "\n",
    "\n",
    "# берем данные для тренировки ranking модели\n",
    "data_train_ranker = data_val_matcher.copy()  # Для наглядности. Далее мы добавим изменения, и они будут отличаться\n",
    "\n",
    "# берем данные для теста ranking, matching модели\n",
    "data_val_ranker = data[data['week_no'] >= data['week_no'].max() - VAL_RANKER_WEEKS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats_data(data_train_matcher,'train_matcher')\n",
    "print_stats_data(data_val_matcher,'val_matcher')\n",
    "print_stats_data(data_train_ranker,'train_ranker')\n",
    "print_stats_data(data_val_ranker,'val_ranker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter the data\n",
    "\n",
    "n_items_before = data_train_matcher['item_id'].nunique()\n",
    "\n",
    "data_train_matcher = prefilter_items(data_train_matcher, item_features=item_features, take_n_popular=20000)\n",
    "\n",
    "n_items_after = data_train_matcher['item_id'].nunique()\n",
    "print('Decreased # items from {} to {}'.format(n_items_before, n_items_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and leave only users who are found in the train dataset\n",
    "\n",
    "common_users = data_train_matcher.user_id.values\n",
    "\n",
    "data_val_matcher = data_val_matcher[data_val_matcher.user_id.isin(common_users)]\n",
    "data_train_ranker = data_train_ranker[data_train_ranker.user_id.isin(common_users)]\n",
    "data_val_ranker = data_val_ranker[data_val_ranker.user_id.isin(common_users)]\n",
    "\n",
    "print_stats_data(data_train_matcher,'train_matcher')\n",
    "print_stats_data(data_val_matcher,'val_matcher')\n",
    "print_stats_data(data_train_ranker,'train_ranker')\n",
    "print_stats_data(data_val_ranker,'val_ranker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-st level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = MainRecommender(data_train_matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUAL_COL = 'actual'\n",
    "result_eval_matcher = data_val_matcher.groupby(USER_COL)[ITEM_COL].unique().reset_index()\n",
    "result_eval_matcher.columns=[USER_COL, ACTUAL_COL]\n",
    "result_eval_matcher.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'own_rec': recommender.get_own_recommendations, \n",
    "          'sim_item_rec': recommender.get_similar_items_recommendation, \n",
    "          'als_rec': recommender.get_als_recommendations, \n",
    "          'sim_user_rec': recommender.get_similar_users_recommendation}\n",
    "\n",
    "for column_name, model in models.items():\n",
    "    result_eval_matcher[column_name] = make_recommendations(result_eval_matcher, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_eval_matcher.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPK_RECALL = 500\n",
    "sorted(calc_recall(result_eval_matcher, TOPK_RECALL), key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generation of features for the second level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# took users from the train dataset for ranking:\n",
    "\n",
    "N_PREDICT = 500\n",
    "\n",
    "df_match_candidates = pd.DataFrame(data_train_ranker[USER_COL].unique())\n",
    "df_match_candidates.columns = [USER_COL]\n",
    "\n",
    "df_match_candidates['candidates'] = make_recommendations(df_match_candidates, recommender.get_own_recommendations, \n",
    "                                                         N_PREDICT=N_PREDICT)\n",
    "\n",
    "df_items = df_match_candidates.apply(lambda x: pd.Series(x['candidates']), axis=1).stack().reset_index(level=1, drop=True)\n",
    "df_items.name = 'item_id'\n",
    "df_match_candidates = df_match_candidates.drop('candidates', axis=1).join(df_items)\n",
    "\n",
    "df_match_candidates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranker_train = data_train_ranker[[USER_COL, ITEM_COL]].copy()\n",
    "df_ranker_train['target'] = 1  # тут только покупки \n",
    "\n",
    "df_ranker_train = df_match_candidates.merge(df_ranker_train, on=[USER_COL, ITEM_COL], how='left')\n",
    "\n",
    "# чистим дубликаты\n",
    "df_ranker_train = df_ranker_train.drop_duplicates(subset=[USER_COL, ITEM_COL])\n",
    "\n",
    "df_ranker_train['target'].fillna(0, inplace= True)\n",
    "\n",
    "df_ranker_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranker_train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranker_train = df_ranker_train.merge(item_features, on='item_id', how='left')\n",
    "df_ranker_train = df_ranker_train.merge(user_features, on='user_id', how='left')\n",
    "\n",
    "df_ranker_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate new features and add them to the training dataset\n",
    "\n",
    "# Let's add a category parameter to the original training dataset for the convenience of creating new features\n",
    "\n",
    "data_department = data_train_ranker.merge(item_features[['item_id', 'department']], on='item_id', how='inner')\n",
    "data_department.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average price of goods purchased by the user\n",
    "\n",
    "users_sales = data_train_ranker.groupby(USER_COL)[['sales_value', 'quantity']].sum().reset_index()\n",
    "users_sales['avg_price'] = users_sales['sales_value'] / users_sales['quantity']\n",
    "df_ranker_train = df_ranker_train.merge(users_sales[['user_id', 'avg_price']], on='user_id', how='left')\n",
    "df_ranker_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of purchases in each category and average purchase amount in each category for a user\n",
    "\n",
    "users_sales_department = data_department.groupby([USER_COL, 'department'])\\\n",
    "                        [['sales_value', 'quantity']].sum().reset_index()\n",
    "users_sales_department.rename(columns={'quantity': 'n_sold_category'}, inplace=True)\n",
    "users_sales_department['avg_transaction_category'] = users_sales_department['sales_value']\\\n",
    "                                                    /users_sales_department['n_sold_category']\n",
    "users_sales_department.drop(columns=['sales_value'], inplace=True)\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(\n",
    "    users_sales_department, on=[USER_COL, 'department'], how='left')\n",
    "df_ranker_train['Missing n_sold_category'] = 0\n",
    "df_ranker_train.loc[df_ranker_train['n_sold_category'].isna(), 'Missing n_sold_category'] = 1\n",
    "df_ranker_train['n_sold_category'].fillna(0, inplace=True)\n",
    "\n",
    "df_ranker_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average purchase amount per category\n",
    "\n",
    "department_sales = data_department.groupby('department')['sales_value'].mean().reset_index()\n",
    "department_sales.rename(columns={'sales_value': 'mean_sales_value_category'}, inplace=True)\n",
    "department_sales.tail(2)\n",
    "\n",
    "n_weeks = data_department['week_no'].max() - data_department['week_no'].min() + 1\n",
    "\n",
    "# Количество покупок юзером конкретной категории в неделю\n",
    "users_department = data_department.groupby([USER_COL, 'department'])['quantity'].sum().reset_index()\n",
    "users_department['quantity'] /= n_weeks\n",
    "users_department.rename(columns={'quantity': 'n_sold_category_user_week'}, inplace=True)\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(department_sales, on='department', how='left')\n",
    "df_ranker_train = df_ranker_train.merge(users_department, on=[USER_COL, 'department'], how='left')\n",
    "df_ranker_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price\n",
    "\n",
    "items_sales = data_department.groupby(ITEM_COL)[['sales_value', 'quantity']].sum().reset_index()\n",
    "items_sales['price'] = items_sales['sales_value'] / items_sales['quantity']\n",
    "items_sales['price'].fillna(0, inplace=True)\n",
    "\n",
    "# Количество покупок товара в неделю\n",
    "items_sales['quantity_per_week'] = items_sales['quantity'] / n_weeks\n",
    "\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(items_sales[[ITEM_COL,'price', 'quantity_per_week']],\n",
    "                                        on=ITEM_COL, how='left')\n",
    "\n",
    "df_ranker_train['Missing price'] = 0\n",
    "df_ranker_train.loc[df_ranker_train['price'].isna(), 'Missing price'] = 1\n",
    "df_ranker_train['price'].fillna(0, inplace=True)\n",
    "\n",
    "df_ranker_train['Missing quantity per week'] = 0\n",
    "df_ranker_train.loc[df_ranker_train['quantity_per_week'].isna(), 'Missing quantity per week'] = 1\n",
    "df_ranker_train['quantity_per_week'].fillna(0, inplace=True)\n",
    "\n",
    "df_ranker_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique stores that sold the item\n",
    "\n",
    "items_stores = data_department.groupby(ITEM_COL)['store_id'].nunique().reset_index()\n",
    "items_stores.rename(columns={'store_id': 'n_unique_stores'}, inplace=True)\n",
    "df_ranker_train = df_ranker_train.merge(items_stores, on=ITEM_COL, how='left')\n",
    "\n",
    "df_ranker_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of customer transactions per week\n",
    "\n",
    "users_transactions = data_department.groupby(USER_COL)[ITEM_COL].count().reset_index()\n",
    "users_transactions.rename(columns={'item_id': 'n_transactions_per_week'}, inplace=True)\n",
    "users_transactions['n_transactions_per_week'] /= n_weeks\n",
    "\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(users_transactions, on=USER_COL, how='left')\n",
    "\n",
    "df_ranker_train.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average check\n",
    "\n",
    "users_sales = data_train_ranker.groupby(USER_COL)['sales_value'].mean().reset_index()\n",
    "users_sales.rename(columns={'sales_value': 'avg_cheque'}, inplace=True)\n",
    "df_ranker_train = df_ranker_train.merge(users_sales[['user_id', 'avg_cheque']], on='user_id', how='left')\n",
    "df_ranker_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of unique categories in cart\n",
    "\n",
    "users_baskets = data_department.groupby([USER_COL, 'basket_id'])['department'].nunique().reset_index()\n",
    "users_baskets = users_baskets.groupby(USER_COL)['department'].mean().reset_index()\n",
    "users_baskets.rename(columns={'department': 'avg_basket_department'}, inplace=True)\n",
    "df_ranker_train = df_ranker_train.merge(users_baskets[['user_id', 'avg_basket_department']], on='user_id', how='left')\n",
    "df_ranker_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create a feature that reflects the average interval between user purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_days = data_department.groupby(USER_COL)['day'].unique().reset_index()\n",
    "users_days['day'] = users_days['day'].apply(lambda x: sorted(x))\n",
    "users_days.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_ndays(days):\n",
    "    diff = 0\n",
    "    if len(days) > 1:\n",
    "        for i in range(len(days) - 1):\n",
    "            diff += days[i+1] - days[i]\n",
    "        return diff / (len(days) - 1)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "users_days['avg_interval'] = users_days['day'].apply(avg_ndays)\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(users_days[['user_id', 'avg_interval']], on='user_id', how='left')\n",
    "df_ranker_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create a feature that will encode the place of the product in the last five purchases of the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_items = data_train_ranker.groupby(USER_COL)[ITEM_COL].apply(list).reset_index()\n",
    "users_items['item_id'] = users_items['item_id'].apply(lambda x: x[-5:])\n",
    "users_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_last_sales(x, df=users_items):\n",
    "    last_sales = df.loc[df['user_id'] == x[0], 'item_id'].item()\n",
    "    code = str()\n",
    "    last_sales.reverse()\n",
    "    for item in last_sales:\n",
    "        code += '1' if item == x[1] else '0'\n",
    "    return code\n",
    "\n",
    "df_ranker_train['Last5sales'] = df_ranker_train[[USER_COL, ITEM_COL]].apply(code_last_sales, axis=1)\n",
    "df_ranker_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create a Word2Vec model to get product embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = data_train_ranker.groupby(USER_COL)[ITEM_COL].unique().reset_index()\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases = []\n",
    "\n",
    "for user in df_['user_id']:\n",
    "    purchases.append([str(item) for item in df_[df_['user_id'] == user].item_id.values[0]])\n",
    "    \n",
    "print(f\"Total # of Sessions: {len(purchases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=1, vector_size=100, sg=1, workers=3)\n",
    "w2v_model.build_vocab(purchases, progress_per=100)\n",
    "w2v_model.train(purchases, total_examples=w2v_model.corpus_count, epochs=12, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_len(itemid):\n",
    "    try:\n",
    "        return sum([i**2 for i in w2v_model.wv[str(itemid)]])\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "df_ranker_train['Word2Vec_length'] = df_ranker_train[ITEM_COL].apply(lambda x: word2vec_len(x))\n",
    "df_ranker_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec(items):\n",
    "    return sum([w2v_model.wv[str(item)] for item in items]) / len(items)\n",
    "\n",
    "df_['Avg_Word2Vec'] = df_[ITEM_COL].apply(avg_word2vec)\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_distance(x, df=df_):\n",
    "    avg_w2v = df.loc[df_[USER_COL] == x[0], 'Avg_Word2Vec'].item()\n",
    "    try:\n",
    "        return sum((w2v_model.wv[str(x[1])] - avg_w2v) ** 2)\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "df_ranker_train['Word2Vec_distance_from_avg'] = df_ranker_train[[USER_COL, ITEM_COL]].\\\n",
    "                                                apply(get_w2v_distance, axis=1)\n",
    "df_ranker_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation a second level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_ranker_train.drop(['target', \n",
    "                                'Missing n_sold_category', \n",
    "                                'n_sold_category_user_week', \n",
    "                                'mean_sales_value_category',], axis=1)\n",
    "y_train = df_ranker_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = ['manufacturer', \n",
    "             'department', \n",
    "             'brand', \n",
    "             'commodity_desc',\n",
    "             'sub_commodity_desc',\n",
    "             'curr_size_of_product',\n",
    "             'age_desc',\n",
    "             'marital_status_code',\n",
    "             'income_desc',\n",
    "             'homeowner_desc',\n",
    "             'hh_comp_desc',\n",
    "             'household_size_desc',\n",
    "             'kid_category_desc',\n",
    "             'Missing price',\n",
    "             'Missing quantity per week',\n",
    "             'Last5sales',\n",
    "            ]\n",
    "\n",
    "\n",
    "for column in cat_feats:\n",
    "    X_train[column].fillna(0, inplace=True)\n",
    "    \n",
    "X_train[cat_feats] = X_train[cat_feats].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cb = CatBoostClassifier(learning_rate=0.1,\n",
    "                        max_depth=12,\n",
    "                        n_estimators=800,\n",
    "                        random_state=42, \n",
    "                        cat_features=cat_feats, \n",
    "                        silent=False)\n",
    "\n",
    "cb.fit(X_train, y_train)\n",
    "\n",
    "train_preds = cb.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame(cb.feature_importances_, index=X_train.columns, columns=['importance'])\n",
    "fi.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranker_predict = df_ranker_train.copy()\n",
    "df_ranker_predict['proba_item_purchase'] = train_preds[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PREDICT = 50\n",
    "TOPK_PRECISION = 5\n",
    "\n",
    "result_eval_ranker = data_val_ranker.groupby(USER_COL)[ITEM_COL].unique().reset_index()\n",
    "result_eval_ranker.columns=[USER_COL, ACTUAL_COL]\n",
    "result_eval_ranker['own_rec'] = make_recommendations(result_eval_ranker, \n",
    "                                                     recommender.get_own_recommendations, N_PREDICT=N_PREDICT)\n",
    "\n",
    "sorted(calc_precision(result_eval_ranker, TOPK_PRECISION), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_eval_ranker['reranked_own_rec'] = result_eval_ranker[USER_COL].\\\n",
    "                                            apply(lambda user_id: rerank(user_id, df_ranker_predict))\n",
    "print(*sorted(calc_precision(result_eval_ranker, TOPK_PRECISION), key=lambda x: x[1], reverse=True), sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
